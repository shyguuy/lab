https://pythonpip.ru/osnovy/tokenizatsiya-python
https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/amp/
https://ru.stackoverflow.com/questions/1519656/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D1%8B-%D0%B4%D0%BB%D1%8F-nlp-%D1%83%D0%B4%D0%B0%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2-%D0%BF%D1%80%D0%B5%D0%BF%D0%B8%D0%BD%D0%B0%D0%BD%D0%B8%D1%8F

import re

def remove_space(text):
    return re.sub(r'\s+',' ', text, flags=re.I) # метод для 
    удаления пробелов

def remove_stopwords(text):
    return [word for word in text if word not in stopwords] # 
    удаление стоп-слов

def tokenize(text):
    t = word_tokenize(text)
    return [token for token in t if token not in stopwords] # 
    метод для токенизации



wn = nltk.WordNetLemmatizer()

def lemmatize(text):
    res = list()
    for word in text:
        p = morph.parse(word)[0]
        res.append(p.normal_form)
    return res # метод для лемматизации

morph = pymorphy2.MorphAnalyzer()
def part(text):
    res = list()
    for word in text:
        p = morph.parse(word)[0]
        a = (word, p.tag.cyr_repr)
        res.append(tuple(a))
    return res # метод для выделения значимых частей речи
######################################################################

    text_list = []

for i in df['text']:
    if i is not None:
        i = remove_punctuation(i)
        i = remove_numbers(i)
        i = remove_notalpha(i)
        i = remove_space(i)
        i = tokenize(i)
        i = remove_stopwords(i)
        i = lemmatize(i)
        text_list.append(i)
######################################################################
        prep_txt_list = []
for txt in text_list:
    prep_txt = part(txt)
    prep_txt_list.append(prep_txt)
######################################################################
from nltk import word_tokenize, FreqDist
from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder

text = "Ваш текст здесь."

# Токенизация текста
tokens = word_tokenize(text)

# Поиск биграмм и триграмм
bigram_finder = BigramCollocationFinder.from_words(tokens)
trigram_finder = TrigramCollocationFinder.from_words(tokens)

# Получение наиболее частых биграмм и триграмм
top_bigrams = bigram_finder.nbest(FreqDist(tokens).max, 10)
top_trigrams = trigram_finder.nbest(FreqDist(tokens).max, 10)

print("Top Bigrams:", top_bigrams)
print("Top Trigrams:", top_trigrams)
###################################################################
import spacy

nlp = spacy.load("ru_core_news_sm")

text = "Ваш текст здесь."

# Обработка текста
doc = nlp(text)

# Извлечение биграмм и триграмм
bigrams = [(token.text, token.nbor.text) for token in doc if token.nbor is not None]
trigrams = [(token.text, token.nbor.text, token.nbor.nbor.text) for token in doc if token.nbor is not None and token.nbor.nbor is not None]

print("Bigrams:", bigrams)
print("Trigrams:", trigrams)
###############################################################
import pandas as pd
from nltk import word_tokenize, FreqDist
from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder

# Предположим, у вас есть DataFrame 'df' с текстовым столбцом 'text'
df['text'] = df['text'].astype(str)

# Функция для извлечения биграмм и триграмм
def extract_ngrams(text):
    tokens = word_tokenize(text)
    bigram_finder = BigramCollocationFinder.from_words(tokens)
    trigram_finder = TrigramCollocationFinder.from_words(tokens)
    top_bigrams = bigram_finder.nbest(FreqDist(tokens).max, 5)  # Выберите количество биграмм
    top_trigrams = trigram_finder.nbest(FreqDist(tokens).max, 5)  # Выберите количество триграмм
    return top_bigrams + top_trigrams

# Применение функции и добавление новых столбцов
df['ngrams'] = df['text'].apply(extract_ngrams)
#############################################################
from sklearn.feature_extraction.text import CountVectorizer

# Пример использования CountVectorizer
corpus = ["Документ 1", "Документ 2", "Документ 3"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
############################################################
from sklearn.feature_extraction.text import TfidfVectorizer

# Пример использования TfidfVectorizer
vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(corpus)
###########################################################
# Пример: тепловая карта корреляции между текстовыми признаками
plt.figure(figsize=(10, 8))
sns.heatmap(df[['временной_признак', 'рейтинг', 'ключевые_слова']].corr(), annot=True, cmap='coolwarm')
plt.title('Тепловая карта корреляции текстовых признаков')
plt.show()
